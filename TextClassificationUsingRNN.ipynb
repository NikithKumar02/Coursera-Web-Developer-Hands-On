{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextClassificationUsingRNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyORirAu4fhzESmkv+hgccN5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikithKumar02/Coursera-Web-Developer-Hands-On/blob/main/TextClassificationUsingRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import the packages"
      ],
      "metadata": {
        "id": "TCeIdLVvIUNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "dL0iatb9Uq9B"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense,Input,Flatten\n",
        "from keras.layers import Conv1D,MaxPooling1D,Embedding\n",
        "from keras.models import Model\n",
        "from keras.utils.np_utils import to_categorical"
      ],
      "metadata": {
        "id": "qooDW5iGITq_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH=1000\n",
        "MAX_NB_WORDS=10000\n",
        "EMBEDDING_DIM=100\n",
        "VALIDATION_SPLIT=0.2"
      ],
      "metadata": {
        "id": "QokwPYMBbEZX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Create the index of word and vector"
      ],
      "metadata": {
        "id": "-X0spKPGupVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index={}\n",
        "f=open('/content/glove.6B.100d.txt','r',encoding='utf-8')\n",
        "for line in f:\n",
        "    values=line.split()\n",
        "    word=values[0]\n",
        "    coefs=np.asarray(values[1:],dtype='float32')\n",
        "    embeddings_index[word]=coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peAqn0_dbUuP",
        "outputId": "6219c3b8-1d0e-4611-e502-46b5507c80c6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# selected category\n"
      ],
      "metadata": {
        "id": "Tlt3nN4NuwLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_categories = [\n",
        "    'comp.graphics',\n",
        "    'rec.motorcycles',\n",
        "    'rec.sport.baseball',\n",
        "    'misc.forsale',\n",
        "    'sci.electronics',\n",
        "    'sci.med',\n",
        "    'talk.politics.guns',\n",
        "    'talk.religion.misc']"
      ],
      "metadata": {
        "id": "uF5z8dVtcL-y"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Data"
      ],
      "metadata": {
        "id": "zxTgkbwNHpmM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ubJhd5PNHYkg"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(newsgroups_train))\n",
        "print(list(newsgroups_train.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dG8HRwnHo8Z",
        "outputId": "d0a893ea-935a-4522-dfc7-b931f110add5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['data', 'filenames', 'target_names', 'target', 'DESCR']\n",
            "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_train = fetch_20newsgroups(subset = 'train', categories = selected_categories, remove = ('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset = 'test', categories = selected_categories, remove=('headers', 'footers', 'quotes'))"
      ],
      "metadata": {
        "id": "MgbbOCnGJHEW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = newsgroups_train['data']\n",
        "labels = newsgroups_train['target']"
      ],
      "metadata": {
        "id": "FvNKzPuIcl2Q"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(texts))\n",
        "print(np.unique(labels))\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1qUEsDCco_q",
        "outputId": "6650761d-193f-48b8-a265-572c49f31767"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4472\n",
            "[0 1 2 3 4 5 6 7]\n",
            "[6 3 0 ... 4 3 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [t for t in texts]\n",
        "print(type(texts[0]),texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50sBkOVNcrgc",
        "outputId": "d2551809-5674-47d5-dd00-3e4002373c5b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text vectorization"
      ],
      "metadata": {
        "id": "-zdXhqbNvY2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer(num_words=MAX_NB_WORDS)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences=tokenizer.texts_to_sequences(texts)"
      ],
      "metadata": {
        "id": "aIQQ-Jsfc9hf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index=tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCKd5QIwc-u7",
        "outputId": "6ea6e7a5-38d6-42d7-85bf-ed89e134e9a2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 43747 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pad_sequences(sequences, maxlen=MAX_NB_WORDS)\n",
        "labels = to_categorical(np.asarray(labels))\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "print('Data validation split.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3q6PYoxdmVc",
        "outputId": "145eb901-be97-4a92-d941-e04cd01ffe75"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of data tensor: (4472, 10000)\n",
            "Shape of label tensor: (4472, 8)\n",
            "Data validation split.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Divide data into training set and validation set"
      ],
      "metadata": {
        "id": "2hjndoSrvddA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indices=np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "num_validation_samples = int(VALIDATION_SPLIT*data.shape[0])\n",
        "x_train = data[:-num_validation_samples]\n",
        "y_train = labels[:-num_validation_samples]\n",
        "x_val = data[-num_validation_samples:]\n",
        "y_val = labels[-num_validation_samples:]"
      ],
      "metadata": {
        "id": "dmTFpc_TdoQV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create word embeddings"
      ],
      "metadata": {
        "id": "ZcwMFDI1vjj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = min(MAX_NB_WORDS, len(word_index))\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i >= MAX_NB_WORDS:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # If the word is not in the word index, set all to 0.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "embedding_layer = Embedding(num_words,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_NB_WORDS,\n",
        "                            trainable=False)\n",
        "print(\"Initializing model...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ8HAgJedxSj",
        "outputId": "7715c85f-f86d-4701-de93-99b40b87c638"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize convolutional and pooling layers\n"
      ],
      "metadata": {
        "id": "efe-aczzvsW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_input = Input(shape = (MAX_NB_WORDS,) ,dtype = 'int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "x = Conv1D(128,5,activation='relu')(embedded_sequences)\n",
        "x = MaxPooling1D(5)(x)\n",
        "x = Conv1D(128,5,activation='relu')(x)\n",
        "x = MaxPooling1D(5)(x)\n",
        "x = Conv1D(128, 5, activation='relu')(x)\n",
        "x = MaxPooling1D(35)(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "preds = Dense(8, activation='softmax')(x)"
      ],
      "metadata": {
        "id": "OvUuN87vd0Ya"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(sequence_input,preds)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])\n",
        "print(\"Training Model...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qeq4NKrLeOld",
        "outputId": "c3793041-a372-4ad0-d6e5-cccee9fd93f0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train Model"
      ],
      "metadata": {
        "id": "WqLamgrOectp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, batch_size = 128, epochs = 2, validation_data = (x_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZRdcBXOegTf",
        "outputId": "6513e0fd-557c-4109-bf2c-254187be872b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "28/28 [==============================] - 305s 11s/step - loss: 1.9866 - acc: 0.1786 - val_loss: 1.9881 - val_acc: 0.1902\n",
            "Epoch 2/2\n",
            "28/28 [==============================] - 304s 11s/step - loss: 1.9482 - acc: 0.1864 - val_loss: 1.9978 - val_acc: 0.1924\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe1d17cee10>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Score & Accuracy"
      ],
      "metadata": {
        "id": "0XAuTF_dvvbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = model.predict(x_val)\n",
        "score, acc = model.evaluate(x_val, y_val, batch_size=128)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlVCl16nlHXX",
        "outputId": "0ae668fc-0f50-474e-ea5a-e9f50fe78615"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 25s 4s/step - loss: 1.9978 - acc: 0.1924\n",
            "Test score: 1.9978173971176147\n",
            "Test accuracy: 0.1923937350511551\n"
          ]
        }
      ]
    }
  ]
}